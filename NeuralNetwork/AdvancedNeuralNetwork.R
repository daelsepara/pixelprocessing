# sigmoid activation function
h_func <- function(x) {
	return(1/(1 + exp(-x)))
}

# 1st-derivative of activation function
h_funcd <- function(x) {
	z = h_func(x)
	return(z * (1 - z))
}

# forward propagation
nnet_forward <- function(training_set, w_ji, w_kj) {
	
	# add bias column to input layer
	x = cbind(array(1, c(nrow(training_set), 1)), training_set)
	
	# compute hidden layer activation
	z_2 = x %*% t(w_ji)
	z_j = h_func(z_2)
	
	# add bias column
	a_2 = cbind(array(1, c(nrow(z_j), 1)), z_j)
	
	# compute output layer
	y_k = h_func(a_2 %*% t(w_kj))
	
	return(list('y_k' = y_k, 'z_2' = z_2, 'a_2' = a_2))
}

# backward propagation
nnet_backprop <- function(training_set, y_k, z_2, a_2, w_ji, w_kj, y_matrix, lambda = 0) {
	
	# add bias column
	x = cbind(array(1, c(nrow(training_set), 1)), training_set)
	m = nrow(x)
	
	# compute intermediate delta values per layer
	d3 = y_k - y_matrix
	d2 = d3 %*% w_kj[, 2:ncol(w_kj)] * h_funcd(z_2)
	
	# compute gradient
	dWji = (t(d2) %*% x) / m
	dWkj = (t(d3) %*% a_2) / m

	# cost function
	cost = sum(-y_matrix * log(y_k) - (1 - y_matrix) * log(1 - y_k)) / m
	
	# regularization on lambda != 0
	if (lambda != 0) {
	
		rWji = w_ji
		rWkj = w_kj
		
		# do not regularize bias column
		rWji[, 1] = array(0, nrow(w_ji))
		rWkj[, 1] = array(0, nrow(w_kj))
		
		cost = cost + lambda*(sum(rWji * rWji) + sum(rWkj * rWkj)) / (2 * m)
		
		dWji = dWji + lambda*rWji / m
		dWkj = dWkj + lambda*rWkj / m
	}

	return(list('dWkj' = dWkj, 'dWji' = dWji, 'Error' = cost))	
}

# neutral network cost function for use with advanced optimization method (fmincg)
nnet_cost <- function(X, P1, P2, P3 , P4, P5, P6) {

	# P1 training_set
	# P2 y_matrix (expected output)
	# P3 number of input units
	# P4 hidden layer units
	# P5 number of labels
	# P6 lambda (regularization)
	
	# roll up vectors into arrays
	offs = P4 * (P3 + 1)
	w_ji = array(X[1:offs], c(P4, P3 + 1))
	w_kj = array(X[(1 + offs):length(X)], c(P5, P4 + 1))

	# compute cost function (J) and its gradients (partial derivatives)
	# using forward and backpropagation
	forward = nnet_forward(P1, w_ji, w_kj)
	result = nnet_backprop(P1, forward$y_k, forward$z_2, forward$a_2, w_ji, w_kj, P2, P6)

	# unroll gradient matrices into one vector
	grad = c(as.vector(result$dWji), as.vector(result$dWkj))

	return(list('J' = result$Error, 'grad' = grad))
}

# network training
nnet_train <-function(maxiter = 100, learning_rate = 0.1, tol = 10^(-3), training_set = array(0) , output = array(0), hidden_units = 0, num_labels = 1, min_max = 1, isGaussian = FALSE) {

	# For multi-classification problem, format expected output
	# i.e. matrix, each row corresponds to a training pattern.
	# Each element in the row-vector is a 0 or 1 indicating whether
	# or not it belongs to that particular class
	if (num_labels > 1) {
		eye_matrix = diag(num_labels)
		y_matrix = eye_matrix[output, ]
	} else {
		# binary classification
		y_matrix = output
	}
	
	# determine network dimensions from user input
	j = hidden_units
	inputs = ncol(training_set)
  
	# intialize interconnection weights with random values (-min_max, min_max) or Gaussian (mean = 0, sd = min_max)
	if (!isGaussian) {
		w_ji = array(runif(n = j * (inputs + 1), min = -min_max, max = min_max), c(j, inputs + 1))
		w_kj = array(runif(n = num_labels * (j + 1), min = -min_max, max = min_max), c(num_labels, j + 1))
	} else {
		w_ji = array(rnorm(n = j * (inputs + 1), mean = 0, sd = abs(min_max)), c(j, inputs + 1))
		w_kj = array(rnorm(n = num_labels * (j + 1), mean = 0, sd = abs(min_max)), c(num_labels, j + 1))
	}
  
	iter = 0
	Error = 1.0
	
	y_k = numeric(0)
	
	while (iter < maxiter && Error > tol) {
	
		# for training, perform forward and backpropagation each iteration, no regularization
		forward = nnet_forward(training_set, w_ji, w_kj)
		backward = nnet_backprop(training_set, forward$y_k, forward$z_2, forward$a_2, w_ji, w_kj, y_matrix, 0)
		
		# update weights (using learning rate and gradient descent)
		w_ji = w_ji - learning_rate * backward$dWji
		w_kj = w_kj - learning_rate * backward$dWkj
		
		# save current performance
		Error = backward$Error
		y_k = forward$y_k
    
		iter = iter + 1
		
		if (iter %% 1000 == 0) {
		  print(paste('iteration = ', iter, ' Error = ', Error))
		}
	}

	# add prediction
	prediction = nnet_predict(training_set, w_ji, w_kj)
	
	return(list('y_k' = y_k, 'Error' = Error, 'iterations' = iter, 'w_kj' = w_kj, 'w_ji' = w_ji, 'prediction' = prediction))
}

# network training (using advanced optimization algorithm fmincg)
nnet_optimize <-function(maxiter = 100, training_set = array(0) , output = array(0), hidden_units = 0, num_labels = 1, min_max = 1, isGaussian = FALSE, lambda = 0) {

	# For multi-classification problem, format expected output
	# i.e. matrix, each row corresponds to a training pattern.
	# Each element in the row-vector is a 0 or 1 indicating whether
	# or not it belongs to that particular class
	if (num_labels > 1) {
		eye_matrix = diag(num_labels)
		y_matrix = eye_matrix[output, ]
	} else {
		# binary classification
		y_matrix = output
	}
	
	# determine network dimensions from user input
	j = hidden_units
	inputs = ncol(training_set)
  
	# intialize interconnection weights with random values (-min_max, min_max) or Gaussian (mean = 0, sd = min_max)
	if (!isGaussian) {
		w_ji = array(runif(n = j * (inputs + 1), min = -min_max, max = min_max), c(j, inputs + 1))
		w_kj = array(runif(n = num_labels * (j + 1), min = -min_max, max = min_max), c(num_labels, j + 1))
	} else {
		w_ji = array(rnorm(n = j * (inputs + 1), mean = 0, sd = abs(min_max)), c(j, inputs + 1))
		w_kj = array(rnorm(n = num_labels * (j + 1), mean = 0, sd = abs(min_max)), c(num_labels, j + 1))
	}
  
	initialWeights = c(as.vector(w_ji), as.vector(w_kj))
	optimizationResult = fmincg(nnet_cost, initialWeights, maxiter, training_set, y_matrix, inputs, j, num_labels, lambda)
	
	offs = j * (inputs + 1)
	w_ji = array(optimizationResult$X[1:offs], c(j, inputs + 1))
	w_kj = array(optimizationResult$X[(1 + offs):length(optimizationResult$X)], c(num_labels, j + 1))
		
	# performance
	Error = optimizationResult$cost
	y_k = nnet_forward(training_set, w_ji, w_kj)$y_k
    
	# add prediction
	prediction = nnet_predict(training_set, w_ji, w_kj)
	
	return(list('y_k' = y_k, 'Error' = Error, 'w_kj' = w_kj, 'w_ji' = w_ji, 'prediction' = prediction))
}

# predict using neural network parameters (multi-class classification)
nnet_predict <- function(test_set, w_ji, w_kj, threshold = 0.5) {

	prediction_output = nnet_forward(test_set, w_ji, w_kj)$y_k
	
	m = nrow(test_set)
	
	prediction = array(0, c(m, 1))
	
	if (ncol(prediction_output) > 1) {
		# for multi-class neural network classifier, each column in
		# the output correspond to a different class. The node (in the output layer)
		# with the highest output value corresponds to its predicted class
		prediction = array(as.integer(apply(prediction_output, 1, which.max)), c(m, 1))
		
	} else {
		# for binary classifier, use threshold to set the output to 0 or 1
		prediction[which(prediction_output > threshold)] = 1
	}
	
	return(prediction)
}
